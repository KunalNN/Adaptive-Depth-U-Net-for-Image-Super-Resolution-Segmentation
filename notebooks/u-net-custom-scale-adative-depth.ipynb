{
 "cells": [
  {
   "attachments": {
    "130a46f2-9046-4106-b324-1983094c922f.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot 2021-11-27 at 4.02.39 PM.png](attachment:130a46f2-9046-4106-b324-1983094c922f.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:03:30.423553Z",
     "iopub.status.busy": "2022-03-14T05:03:30.422939Z",
     "iopub.status.idle": "2022-03-14T05:03:30.431662Z",
     "shell.execute_reply": "2022-03-14T05:03:30.430808Z",
     "shell.execute_reply.started": "2022-03-14T05:03:30.423517Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.1\n"
     ]
    }
   ],
   "source": [
    "import os, re, glob, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from scipy import ndimage\n",
    "from skimage.transform import resize, rescale\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.applications import VGG19\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Conv2D, MaxPooling2D, MaxPool2D, Dropout, Activation,\n",
    "    Conv2DTranspose, UpSampling2D, add, BatchNormalization,\n",
    "    Concatenate, LeakyReLU\n",
    ")\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.saving import register_keras_serializable\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "attachments": {
    "8dcd3305-b356-4f8e-ba85-662cf89fbe62.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot 2021-11-27 at 4.04.31 PM.png](attachment:8dcd3305-b356-4f8e-ba85-662cf89fbe62.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:03:30.434227Z",
     "iopub.status.busy": "2022-03-14T05:03:30.433418Z",
     "iopub.status.idle": "2022-03-14T05:03:37.646655Z",
     "shell.execute_reply": "2022-03-14T05:03:37.645899Z",
     "shell.execute_reply.started": "2022-03-14T05:03:30.434189Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 855/855 [00:01<00:00, 449.85it/s]\n",
      "100%|██████████| 855/855 [00:01<00:00, 495.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training images: (700, 256, 256, 3)\n",
      "Shape of test images: (45, 256, 256, 3)\n",
      "Shape of validation images: (110, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# to get the files in proper order\n",
    "def sorted_alphanumeric(data):  \n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)',key)]\n",
    "    return sorted(data,key = alphanum_key)\n",
    "# defining the size of the image\n",
    "SIZE = 256\n",
    "high_img = []\n",
    "path = '/Users/kunalnarwani/Desktop/Thesis/super-resolution/dataset/Raw Data/high_res' \n",
    "files = os.listdir(path)\n",
    "files = sorted_alphanumeric(files)\n",
    "for i in tqdm(files):    \n",
    "    if i == '855.png':\n",
    "        break\n",
    "    else:    \n",
    "        img = cv2.imread(path + '/'+i,1)\n",
    "        # open cv reads images in BGR format so we have to convert it to RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        #resizing image\n",
    "        img = cv2.resize(img, (SIZE, SIZE))\n",
    "        img = img.astype('float32') / 255.0\n",
    "        high_img.append(img_to_array(img))\n",
    "\n",
    "\n",
    "low_img = []\n",
    "path = '/Users/kunalnarwani/Desktop/Thesis/super-resolution/dataset/Raw Data/low_res'\n",
    "files = os.listdir(path)\n",
    "files = sorted_alphanumeric(files)\n",
    "for i in tqdm(files):\n",
    "    if i == '855.png':\n",
    "        break\n",
    "    else: \n",
    "        img = cv2.imread(path + '/'+i,1)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        #resizing image\n",
    "        img = cv2.resize(img, (SIZE, SIZE))\n",
    "        img = img.astype('float32') / 255.0\n",
    "        low_img.append(img_to_array(img))\n",
    "\n",
    "train_high_image = high_img[:700]\n",
    "train_low_image = low_img[:700]\n",
    "train_high_image = np.reshape(train_high_image,(len(train_high_image),SIZE,SIZE,3))\n",
    "train_low_image = np.reshape(train_low_image,(len(train_low_image),SIZE,SIZE,3))\n",
    "\n",
    "validation_high_image = high_img[700:810]\n",
    "validation_low_image = low_img[700:810]\n",
    "validation_high_image= np.reshape(validation_high_image,(len(validation_high_image),SIZE,SIZE,3))\n",
    "validation_low_image = np.reshape(validation_low_image,(len(validation_low_image),SIZE,SIZE,3))\n",
    "\n",
    "\n",
    "test_high_image = high_img[810:]\n",
    "test_low_image = low_img[810:]\n",
    "test_high_image= np.reshape(test_high_image,(len(test_high_image),SIZE,SIZE,3))\n",
    "test_low_image = np.reshape(test_low_image,(len(test_low_image),SIZE,SIZE,3))\n",
    "\n",
    "print(\"Shape of training images:\",train_high_image.shape)\n",
    "print(\"Shape of test images:\",test_high_image.shape)\n",
    "print(\"Shape of validation images:\",validation_high_image.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARCITECTURE OF MODEL \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:03:39.192734Z",
     "iopub.status.busy": "2022-03-14T05:03:39.192290Z",
     "iopub.status.idle": "2022-03-14T05:03:40.697996Z",
     "shell.execute_reply": "2022-03-14T05:03:40.696721Z",
     "shell.execute_reply.started": "2022-03-14T05:03:39.192694Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scale': 0.5, 'levels': 4, 'L_cap': 6}\n",
      "Sanity out: (1, 256, 256, 3)\n",
      "Model: \"U-Net_SR_tied_s0.50_L4_cap6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 256, 256, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 256, 256, 64)         1792      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (None, 256, 256, 64)         128       ['conv2d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 256, 256, 64)         0         ['layer_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, 256, 256, 64)         128       ['conv2d_1[0][0]']            \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 256, 256, 64)         0         ['layer_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " enc_down (ResizeByScale)    multiple                     0         ['activation_1[0][0]',        \n",
      "                                                                     'activation_3[0][0]',        \n",
      "                                                                     'activation_5[0][0]',        \n",
      "                                                                     'activation_7[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['enc_down[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_2 (Lay  (None, 128, 128, 128)        256       ['conv2d_2[0][0]']            \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 128, 128, 128)        0         ['layer_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " layer_normalization_3 (Lay  (None, 128, 128, 128)        256       ['conv2d_3[0][0]']            \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_3 (Activation)   (None, 128, 128, 128)        0         ['layer_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['enc_down[1][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_4 (Lay  (None, 64, 64, 256)          512       ['conv2d_4[0][0]']            \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 64, 64, 256)          0         ['layer_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " layer_normalization_5 (Lay  (None, 64, 64, 256)          512       ['conv2d_5[0][0]']            \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, 64, 64, 256)          0         ['layer_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['enc_down[2][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_6 (Lay  (None, 32, 32, 512)          1024      ['conv2d_6[0][0]']            \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_6 (Activation)   (None, 32, 32, 512)          0         ['layer_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        \n",
      "                                                                                                  \n",
      " layer_normalization_7 (Lay  (None, 32, 32, 512)          1024      ['conv2d_7[0][0]']            \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_7 (Activation)   (None, 32, 32, 512)          0         ['layer_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['enc_down[3][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_8 (Lay  (None, 16, 16, 1024)         2048      ['conv2d_8[0][0]']            \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_8 (Activation)   (None, 16, 16, 1024)         0         ['layer_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        \n",
      "                                                                                                  \n",
      " layer_normalization_9 (Lay  (None, 16, 16, 1024)         2048      ['conv2d_9[0][0]']            \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_9 (Activation)   (None, 16, 16, 1024)         0         ['layer_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dec_up (ResizeToMatch)      multiple                     0         ['activation_9[0][0]',        \n",
      "                                                                     'activation_7[0][0]',        \n",
      "                                                                     'activation_11[0][0]',       \n",
      "                                                                     'activation_5[0][0]',        \n",
      "                                                                     'activation_13[0][0]',       \n",
      "                                                                     'activation_3[0][0]',        \n",
      "                                                                     'activation_15[0][0]',       \n",
      "                                                                     'activation_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, 32, 32, 512)          4719104   ['dec_up[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 32, 32, 1024)         0         ['conv2d_10[0][0]',           \n",
      "                                                                     'activation_7[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, 32, 32, 512)          4719104   ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_10 (La  (None, 32, 32, 512)          1024      ['conv2d_11[0][0]']           \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_10 (Activation)  (None, 32, 32, 512)          0         ['layer_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       \n",
      "                                                                                                  \n",
      " layer_normalization_11 (La  (None, 32, 32, 512)          1024      ['conv2d_12[0][0]']           \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_11 (Activation)  (None, 32, 32, 512)          0         ['layer_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, 64, 64, 256)          1179904   ['dec_up[1][0]']              \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 64, 64, 512)          0         ['conv2d_13[0][0]',           \n",
      " )                                                                   'activation_5[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, 64, 64, 256)          1179904   ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      " layer_normalization_12 (La  (None, 64, 64, 256)          512       ['conv2d_14[0][0]']           \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_12 (Activation)  (None, 64, 64, 256)          0         ['layer_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       \n",
      "                                                                                                  \n",
      " layer_normalization_13 (La  (None, 64, 64, 256)          512       ['conv2d_15[0][0]']           \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_13 (Activation)  (None, 64, 64, 256)          0         ['layer_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)          (None, 128, 128, 128)        295040    ['dec_up[2][0]']              \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 128, 128, 256)        0         ['conv2d_16[0][0]',           \n",
      " )                                                                   'activation_3[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)          (None, 128, 128, 128)        295040    ['concatenate_2[0][0]']       \n",
      "                                                                                                  \n",
      " layer_normalization_14 (La  (None, 128, 128, 128)        256       ['conv2d_17[0][0]']           \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_14 (Activation)  (None, 128, 128, 128)        0         ['layer_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       \n",
      "                                                                                                  \n",
      " layer_normalization_15 (La  (None, 128, 128, 128)        256       ['conv2d_18[0][0]']           \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_15 (Activation)  (None, 128, 128, 128)        0         ['layer_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)          (None, 256, 256, 64)         73792     ['dec_up[3][0]']              \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate  (None, 256, 256, 128)        0         ['conv2d_19[0][0]',           \n",
      " )                                                                   'activation_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)          (None, 256, 256, 64)         73792     ['concatenate_3[0][0]']       \n",
      "                                                                                                  \n",
      " layer_normalization_16 (La  (None, 256, 256, 64)         128       ['conv2d_20[0][0]']           \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_16 (Activation)  (None, 256, 256, 64)         0         ['layer_normalization_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       \n",
      "                                                                                                  \n",
      " layer_normalization_17 (La  (None, 256, 256, 64)         128       ['conv2d_21[0][0]']           \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_17 (Activation)  (None, 256, 256, 64)         0         ['layer_normalization_17[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_17[0][0]']       \n",
      "                                                                                                  \n",
      " layer_normalization_18 (La  (None, 256, 256, 64)         128       ['conv2d_22[0][0]']           \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_18 (Activation)  (None, 256, 256, 64)         0         ['layer_normalization_18[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_18[0][0]']       \n",
      "                                                                                                  \n",
      " layer_normalization_19 (La  (None, 256, 256, 64)         128       ['conv2d_23[0][0]']           \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_19 (Activation)  (None, 256, 256, 64)         0         ['layer_normalization_19[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " residual_rgb (Conv2D)       (None, 256, 256, 3)          195       ['activation_19[0][0]']       \n",
      "                                                                                                  \n",
      " enhanced_rgb (ClipAdd)      (None, 256, 256, 3)          0         ['input_1[0][0]',             \n",
      "                                                                     'residual_rgb[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34599363 (131.99 MB)\n",
      "Trainable params: 34599363 (131.99 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "\n",
    "# ---------- Resize layers ----------\n",
    "@register_keras_serializable(package=\"resize\")\n",
    "class ResizeByScale(L.Layer):\n",
    "    def __init__(self, scale, method=\"bilinear\", antialias=True, name=None, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.scale = float(scale); self.method = method; self.antialias = antialias\n",
    "    def call(self, x):\n",
    "        x_dtype = x.dtype\n",
    "        h = tf.shape(x)[1]; w = tf.shape(x)[2]\n",
    "        nh = tf.cast(tf.round(tf.cast(h, tf.float32) * self.scale), tf.int32)\n",
    "        nw = tf.cast(tf.round(tf.cast(w, tf.float32) * self.scale), tf.int32)\n",
    "        y = tf.image.resize(tf.cast(x, tf.float32), [nh, nw], method=self.method, antialias=self.antialias)\n",
    "        return tf.cast(y, x_dtype)\n",
    "    def get_config(self):\n",
    "        return {**super().get_config(), \"scale\": self.scale, \"method\": self.method, \"antialias\": self.antialias}\n",
    "\n",
    "@register_keras_serializable(package=\"resize\")\n",
    "class ResizeToMatch(L.Layer):\n",
    "    def __init__(self, method=\"bilinear\", antialias=True, name=None, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.method = method; self.antialias = antialias\n",
    "    def call(self, inputs):\n",
    "        x, ref = inputs\n",
    "        target = tf.shape(ref)[1:3]\n",
    "        y = tf.image.resize(tf.cast(x, tf.float32), target, method=self.method, antialias=self.antialias)\n",
    "        return tf.cast(y, x.dtype)\n",
    "    def get_config(self):\n",
    "        return {**super().get_config(), \"method\": self.method, \"antialias\": self.antialias}\n",
    "\n",
    "# ---------- Core ----------\n",
    "def conv_block(inputs, nf):\n",
    "    x = L.Conv2D(nf, 3, padding=\"same\", use_bias=True)(inputs)\n",
    "    x = L.LayerNormalization(axis=-1)(x); x = L.Activation(\"relu\")(x)\n",
    "    x = L.Conv2D(nf, 3, padding=\"same\", use_bias=True)(x)\n",
    "    x = L.LayerNormalization(axis=-1)(x); x = L.Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "@register_keras_serializable(package=\"utils\")\n",
    "class ClipAdd(L.Layer):\n",
    "    def call(self, inputs):\n",
    "        inp, r = inputs\n",
    "        y = tf.cast(inp, tf.float32) + tf.cast(r, tf.float32)\n",
    "        return tf.cast(tf.clip_by_value(y, 0.0, 1.0), inp.dtype)\n",
    "\n",
    "# ---------- Depth rule: per_level_down == scale ----------\n",
    "HR = 256\n",
    "MIN_BOTTLENECK = 16\n",
    "\n",
    "def max_levels_equal(scale: float, hr: int = HR, min_bottleneck: int = MIN_BOTTLENECK, L_cap: int = 5) -> int:\n",
    "    \"\"\"Levels when every downsample uses the SAME scale factor (per_level_down==scale), with safety cap.\"\"\"\n",
    "    h = hr\n",
    "    L_levels = 0\n",
    "    while int(math.floor(h * scale)) >= min_bottleneck and L_levels < L_cap:\n",
    "        h = int(math.floor(h * scale))\n",
    "        L_levels += 1\n",
    "    return max(L_levels, 1)\n",
    "\n",
    "# ---------- Encoder/Decoder blocks using 'scale' per level ----------\n",
    "def encoder_block(inputs, nf, down_layer):\n",
    "    x = conv_block(inputs, nf)\n",
    "    p = down_layer(x)          # shrink by 'scale'\n",
    "    return x, p\n",
    "\n",
    "def decoder_block(inputs, skip_features, nf, up_layer):\n",
    "    x = up_layer([inputs, skip_features])   # upsize to skip size\n",
    "    x = L.Conv2D(nf, 3, padding='same', activation='relu')(x)\n",
    "    x = L.Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, nf)\n",
    "    return x\n",
    "\n",
    "# ---------- Builder (per_level_down == scale) ----------\n",
    "def build_super_resolution_unet(scale: float,\n",
    "                                     base_channels: int = 64,\n",
    "                                     residual_head_channels: int = 64,\n",
    "                                     L_cap: int = 5):\n",
    "    assert 0.1 <= scale <= 0.99, \"Use 0<scale<1; near-1 requires a small L_cap.\"\n",
    "    L_levels = max_levels_equal(scale, hr=HR, min_bottleneck=MIN_BOTTLENECK, L_cap=L_cap)\n",
    "\n",
    "    down = ResizeByScale(scale,  name=\"enc_down\")\n",
    "    up_to = ResizeToMatch(name=\"dec_up\")\n",
    "\n",
    "    inputs = Input(shape=(HR, HR, 3))\n",
    "    skips = []\n",
    "    x = inputs\n",
    "\n",
    "    # Encoder\n",
    "    nf = base_channels\n",
    "    for _ in range(L_levels):\n",
    "        s, x = encoder_block(x, nf, down_layer=down)\n",
    "        skips.append(s)\n",
    "        nf *= 2\n",
    "\n",
    "    # Bottleneck\n",
    "    x = conv_block(x, nf)\n",
    "\n",
    "    # Decoder\n",
    "    for s in reversed(skips):\n",
    "        nf //= 2\n",
    "        x = decoder_block(x, s, nf, up_layer=up_to)\n",
    "\n",
    "    # Residual head (zero-init for identity start)\n",
    "    xh  = conv_block(x, residual_head_channels)\n",
    "    res = L.Conv2D(3, 1, padding=\"same\", kernel_initializer=\"zeros\", bias_initializer=\"zeros\", name=\"residual_rgb\")(xh)\n",
    "    out = ClipAdd(name=\"enhanced_rgb\")([inputs, res])\n",
    "\n",
    "    model = Model(inputs, out, name=f\"U-Net_SR_tied_s{scale:.2f}_L{L_levels}_cap{L_cap}\")\n",
    "    info = {\"scale\": scale, \"levels\": L_levels, \"L_cap\": L_cap}\n",
    "    return model, info\n",
    "\n",
    "# ======================= Quick sanity check =======================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.keras.backend.clear_session()\n",
    "    model, info = build_super_resolution_unet(scale=0.5, base_channels=64, L_cap=6)\n",
    "    print(info)\n",
    "    print(\"Sanity out:\", y.shape)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:03:40.700266Z",
     "iopub.status.busy": "2022-03-14T05:03:40.699702Z",
     "iopub.status.idle": "2022-03-14T05:03:40.704884Z",
     "shell.execute_reply": "2022-03-14T05:03:40.704018Z",
     "shell.execute_reply.started": "2022-03-14T05:03:40.700229Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "\n",
    "vgg = VGG19(include_top=False, weights=\"imagenet\", input_shape=(256, 256, 3))\n",
    "vgg.trainable = False\n",
    "feat_extractor = tf.keras.Model(\n",
    "    inputs=vgg.input,\n",
    "    outputs=vgg.get_layer(\"block4_conv4\").output,\n",
    ")\n",
    "\n",
    "alpha = tf.constant(1.0,  dtype=tf.float32)\n",
    "beta  = tf.constant(0.1,  dtype=tf.float32)\n",
    "gamma = tf.constant(0.01, dtype=tf.float32)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    val = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    return tf.cast(val, tf.float32)\n",
    "\n",
    "def ssim_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    val = 1.0 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=1.0))\n",
    "    return tf.cast(val, tf.float32)\n",
    "\n",
    "def perceptual_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(tf.clip_by_value(y_true, 0.0, 1.0), tf.float32)\n",
    "    y_pred = tf.cast(tf.clip_by_value(y_pred, 0.0, 1.0), tf.float32)\n",
    "    feat_true = feat_extractor(preprocess_input(y_true * 255.0))\n",
    "    feat_pred = feat_extractor(preprocess_input(y_pred * 255.0))\n",
    "    val = tf.reduce_mean(tf.square(feat_true - feat_pred))\n",
    "    return tf.cast(val, tf.float32)\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    total = (\n",
    "        alpha * mse_loss(y_true, y_pred)\n",
    "        + beta  * ssim_loss(y_true, y_pred)\n",
    "        + gamma * perceptual_loss(y_true, y_pred)\n",
    "    )\n",
    "    return tf.cast(total, tf.float32)\n",
    "\n",
    "def psnr_metric(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(tf.clip_by_value(y_pred, 0.0, 1.0), tf.float32)\n",
    "    return tf.reduce_mean(tf.image.psnr(y_true, y_pred, max_val=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python exec: /Users/kunalnarwani/Desktop/Thesis/super-resolution/.venv/bin/python\n",
      "TF version: 2.15.1\n",
      "GPUs: []\n"
     ]
    }
   ],
   "source": [
    "# ============================== Optimizer / compile ========================= #\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# ===== Compile =====\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=[psnr_metric],\n",
    ")\n",
    "\n",
    "print(\"python exec:\", sys.executable)\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"GPUs:\", tf.config.list_physical_devices(\"GPU\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:03:41.632394Z",
     "iopub.status.busy": "2022-03-14T05:03:41.631893Z",
     "iopub.status.idle": "2022-03-14T05:10:07.166952Z",
     "shell.execute_reply": "2022-03-14T05:10:07.166144Z",
     "shell.execute_reply.started": "2022-03-14T05:03:41.632348Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss did not improve from inf\n",
      "175/175 - 565s - loss: nan - psnr_metric: 29.8278 - val_loss: nan - val_psnr_metric: 30.1294 - 565s/epoch - 3s/step\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 41\u001b[0m\n\u001b[1;32m     31\u001b[0m model_ckpt \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\n\u001b[1;32m     32\u001b[0m     filepath\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(MODEL_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_by_val_loss_0.6.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     33\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     39\u001b[0m backup_cb \u001b[38;5;241m=\u001b[39m BackupAndRestore(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(MODEL_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_backup\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 41\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_ckpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackup_cb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, BackupAndRestore\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 100\n",
    "SEED = 42  # optional, keeps shuffle deterministic across restarts\n",
    "\n",
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((train_low_image, train_high_image))\n",
    "      .shuffle(len(train_low_image), seed=SEED, reshuffle_each_iteration=True)\n",
    "      .batch(BATCH_SIZE)\n",
    "      .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "valid_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((validation_low_image, validation_high_image))\n",
    "      .batch(BATCH_SIZE)\n",
    "      .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "MODEL_DIR = \"/Users/kunalnarwani/Desktop/Thesis/super-resolution/models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "model_ckpt = ModelCheckpoint(\n",
    "    filepath=os.path.join(MODEL_DIR, \"best_by_val_loss_0.6.keras\"),\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_best_only=True,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "backup_cb = BackupAndRestore(os.path.join(MODEL_DIR, \"train_backup\"))\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_ds,\n",
    "    callbacks=[early_stop, model_ckpt, backup_cb],\n",
    "    verbose=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:10:08.297239Z",
     "iopub.status.busy": "2022-03-14T05:10:08.297006Z",
     "iopub.status.idle": "2022-03-14T05:10:08.304194Z",
     "shell.execute_reply": "2022-03-14T05:10:08.303442Z",
     "shell.execute_reply.started": "2022-03-14T05:10:08.297212Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation images evaluated: 110\n",
      " PSNR    : 31.1567 ± 3.5405 dB\n",
      " SSIM    : 0.9250 ± 0.0430\n",
      " MS-SSIM : 0.9939 ± 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 13:03:31.641486: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "EVAL_BATCH = 8  # adjust for GPU/CPU memory\n",
    "eval_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((validation_low_image, validation_high_image))\n",
    "      .batch(EVAL_BATCH)\n",
    "      .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "all_psnr, all_ssim, all_msssim = [], [], []\n",
    "n_images = 0\n",
    "\n",
    "for lr_b, hr_b in eval_ds:\n",
    "    pred_b = model(lr_b, training=False)\n",
    "\n",
    "    if pred_b.shape[1:3] != hr_b.shape[1:3]:  # still guards future changes\n",
    "        pred_b = tf.image.resize(pred_b, size=hr_b.shape[1:3], method=\"bicubic\")\n",
    "\n",
    "    hr_tf   = tf.cast(hr_b, tf.float32)\n",
    "    pred_tf = tf.cast(tf.clip_by_value(pred_b, 0.0, 1.0), tf.float32)\n",
    "\n",
    "    all_psnr.append(tf.image.psnr(hr_tf, pred_tf, max_val=1.0).numpy())\n",
    "    all_ssim.append(tf.image.ssim(hr_tf, pred_tf, max_val=1.0).numpy())\n",
    "    all_msssim.append(tf.image.ssim_multiscale(hr_tf, pred_tf, max_val=1.0).numpy())\n",
    "\n",
    "    n_images += int(hr_b.shape[0])\n",
    "\n",
    "def mean_std(x):\n",
    "    x = np.concatenate(x, axis=0).astype(np.float64)\n",
    "    return float(np.mean(x)), float(np.std(x))\n",
    "\n",
    "m_psnr, s_psnr   = mean_std(all_psnr)\n",
    "m_ssim, s_ssim   = mean_std(all_ssim)\n",
    "m_msssim, s_msssim = mean_std(all_msssim)\n",
    "\n",
    "print(f\"Validation images evaluated: {n_images}\")\n",
    "print(f\" PSNR    : {m_psnr:.4f} ± {s_psnr:.4f} dB\")\n",
    "print(f\" SSIM    : {m_ssim:.4f} ± {s_ssim:.4f}\")\n",
    "print(f\" MS-SSIM : {m_msssim:.4f} ± {s_msssim:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
