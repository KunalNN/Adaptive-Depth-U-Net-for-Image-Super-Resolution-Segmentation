{
 "cells": [
  {
   "attachments": {
    "130a46f2-9046-4106-b324-1983094c922f.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot 2021-11-27 at 4.02.39 PM.png](attachment:130a46f2-9046-4106-b324-1983094c922f.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:03:30.423553Z",
     "iopub.status.busy": "2022-03-14T05:03:30.422939Z",
     "iopub.status.idle": "2022-03-14T05:03:30.431662Z",
     "shell.execute_reply": "2022-03-14T05:03:30.430808Z",
     "shell.execute_reply.started": "2022-03-14T05:03:30.423517Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.1\n"
     ]
    }
   ],
   "source": [
    "import os, re, glob, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from scipy import ndimage\n",
    "from skimage.transform import resize, rescale\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.applications import VGG19\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Conv2D, MaxPooling2D, MaxPool2D, Dropout, Activation,\n",
    "    Conv2DTranspose, UpSampling2D, add, BatchNormalization,\n",
    "    Concatenate, LeakyReLU\n",
    ")\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.saving import register_keras_serializable\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "attachments": {
    "8dcd3305-b356-4f8e-ba85-662cf89fbe62.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot 2021-11-27 at 4.04.31 PM.png](attachment:8dcd3305-b356-4f8e-ba85-662cf89fbe62.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:03:30.434227Z",
     "iopub.status.busy": "2022-03-14T05:03:30.433418Z",
     "iopub.status.idle": "2022-03-14T05:03:37.646655Z",
     "shell.execute_reply": "2022-03-14T05:03:37.645899Z",
     "shell.execute_reply.started": "2022-03-14T05:03:30.434189Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 855/855 [00:22<00:00, 38.63it/s] \n",
      "100%|██████████| 855/855 [00:30<00:00, 28.27it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training images: (700, 256, 256, 3)\n",
      "Shape of test images: (45, 256, 256, 3)\n",
      "Shape of validation images: (110, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# to get the files in proper order\n",
    "def sorted_alphanumeric(data):  \n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)',key)]\n",
    "    return sorted(data,key = alphanum_key)\n",
    "# defining the size of the image\n",
    "SIZE = 256\n",
    "high_img = []\n",
    "path = '/Users/kunalnarwani/Desktop/Thesis/super-resolution/dataset/Raw Data/high_res' \n",
    "files = os.listdir(path)\n",
    "files = sorted_alphanumeric(files)\n",
    "for i in tqdm(files):    \n",
    "    if i == '855.png':\n",
    "        break\n",
    "    else:    \n",
    "        img = cv2.imread(path + '/'+i,1)\n",
    "        # open cv reads images in BGR format so we have to convert it to RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        #resizing image\n",
    "        img = cv2.resize(img, (SIZE, SIZE))\n",
    "        img = img.astype('float32') / 255.0\n",
    "        high_img.append(img_to_array(img))\n",
    "\n",
    "\n",
    "low_img = []\n",
    "path = '/Users/kunalnarwani/Desktop/Thesis/super-resolution/dataset/Raw Data/low_res'\n",
    "files = os.listdir(path)\n",
    "files = sorted_alphanumeric(files)\n",
    "for i in tqdm(files):\n",
    "    if i == '855.png':\n",
    "        break\n",
    "    else: \n",
    "        img = cv2.imread(path + '/'+i,1)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        #resizing image\n",
    "        img = cv2.resize(img, (SIZE, SIZE))\n",
    "        img = img.astype('float32') / 255.0\n",
    "        low_img.append(img_to_array(img))\n",
    "\n",
    "train_high_image = high_img[:700]\n",
    "train_low_image = low_img[:700]\n",
    "train_high_image = np.reshape(train_high_image,(len(train_high_image),SIZE,SIZE,3))\n",
    "train_low_image = np.reshape(train_low_image,(len(train_low_image),SIZE,SIZE,3))\n",
    "\n",
    "validation_high_image = high_img[700:810]\n",
    "validation_low_image = low_img[700:810]\n",
    "validation_high_image= np.reshape(validation_high_image,(len(validation_high_image),SIZE,SIZE,3))\n",
    "validation_low_image = np.reshape(validation_low_image,(len(validation_low_image),SIZE,SIZE,3))\n",
    "\n",
    "\n",
    "test_high_image = high_img[810:]\n",
    "test_low_image = low_img[810:]\n",
    "test_high_image= np.reshape(test_high_image,(len(test_high_image),SIZE,SIZE,3))\n",
    "test_low_image = np.reshape(test_low_image,(len(test_low_image),SIZE,SIZE,3))\n",
    "\n",
    "print(\"Shape of training images:\",train_high_image.shape)\n",
    "print(\"Shape of test images:\",test_high_image.shape)\n",
    "print(\"Shape of validation images:\",validation_high_image.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARCITECTURE OF MODEL \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:03:39.192734Z",
     "iopub.status.busy": "2022-03-14T05:03:39.192290Z",
     "iopub.status.idle": "2022-03-14T05:03:40.697996Z",
     "shell.execute_reply": "2022-03-14T05:03:40.696721Z",
     "shell.execute_reply.started": "2022-03-14T05:03:39.192694Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Prereqs (keep once in your file) ===\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "\n",
    "# =========================== Global Mixed Precision ==========================\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# ============================ Custom Resize Layers ========================== #\n",
    "@register_keras_serializable(package=\"resize\")\n",
    "class ResizeByScale(L.Layer):\n",
    "    \"\"\"Resize spatial dims (H,W) by a float scale factor using tf.image.resize.\"\"\"\n",
    "    def __init__(self, scale, method=\"bilinear\", antialias=True, name=None, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.scale = float(scale)\n",
    "        self.method = method\n",
    "        self.antialias = antialias\n",
    "\n",
    "    def call(self, x):\n",
    "        x_dtype = x.dtype\n",
    "        x32 = tf.cast(x, tf.float32)                      # -> float32 for stable resize + grads\n",
    "        h = tf.shape(x32)[1]; w = tf.shape(x32)[2]\n",
    "        new_h = tf.cast(tf.round(tf.cast(h, tf.float32) * self.scale), tf.int32)\n",
    "        new_w = tf.cast(tf.round(tf.cast(w, tf.float32) * self.scale), tf.int32)\n",
    "        y32 = tf.image.resize(x32, size=[new_h, new_w], method=self.method, antialias=self.antialias)\n",
    "        return tf.cast(y32, x_dtype)                      # back to policy dtype (likely float16)\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\"scale\": float(self.scale), \"method\": self.method, \"antialias\": self.antialias})\n",
    "        return cfg\n",
    "\n",
    "\n",
    "@register_keras_serializable(package=\"resize\")\n",
    "class ResizeToMatch(L.Layer):\n",
    "    \"\"\"Resize x to match the spatial size (H,W) of ref.\"\"\"\n",
    "    def __init__(self, method=\"bilinear\", antialias=True, name=None, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.method = method\n",
    "        self.antialias = antialias\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, ref = inputs\n",
    "        x_dtype = x.dtype\n",
    "        x32  = tf.cast(x, tf.float32)                     # ensure float32 compute\n",
    "        # we only need ref's size; cast avoids mixed dtypes in internal ops\n",
    "        ref32 = tf.cast(ref, tf.float32)\n",
    "        target = tf.shape(ref32)[1:3]\n",
    "        y32 = tf.image.resize(x32, size=target, method=self.method, antialias=self.antialias)\n",
    "        return tf.cast(y32, x_dtype)\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\"method\": self.method, \"antialias\": self.antialias})\n",
    "        return cfg\n",
    "# ============================== Core Blocks ================================= #\n",
    "\n",
    "def conv_block(inputs, num_filters):\n",
    "    x = L.Conv2D(num_filters, 3, padding=\"same\")(inputs)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Activation(\"relu\")(x)\n",
    "    x = L.Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "def _normalize_downscales(downscales):\n",
    "    \"\"\"Ensure downscales is a list of 4 floats (one per encoder level).\"\"\"\n",
    "    if isinstance(downscales, (float, int)):\n",
    "        return [float(downscales)] * 4\n",
    "    if isinstance(downscales, (list, tuple)):\n",
    "        if len(downscales) != 4:\n",
    "            raise ValueError(f\"`downscales` must have length 4; got {len(downscales)}\")\n",
    "        return [float(x) for x in downscales]\n",
    "    raise TypeError(\"`downscales` must be a float or a list/tuple of 4 floats.\")\n",
    "\n",
    "def encoder_block(inputs, num_filters, down_layer):\n",
    "    x = conv_block(inputs, num_filters)\n",
    "    p = down_layer(x)  # per-level downscale\n",
    "    return x, p\n",
    "\n",
    "def decoder_block(inputs, skip_features, num_filters, up_to_layer):\n",
    "    x = up_to_layer([inputs, skip_features])\n",
    "    x = L.Conv2D(num_filters, 3, padding='same', activation='relu')(x)\n",
    "    x = L.Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "# ============================== Model Builder =============================== #\n",
    "\n",
    "def build_super_resolution_unet(input_shape=(256, 256, 3), downscales=0.5, name=None):\n",
    "    \"\"\"\n",
    "    Build U-Net with either a single downscale (applied at all 4 encoder levels)\n",
    "    or a list of 4 different downscales (one per level), e.g. [0.25, 0.5, 0.75, 0.5].\n",
    "    \"\"\"\n",
    "    ds = _normalize_downscales(downscales)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    down_layers = [\n",
    "        ResizeByScale(ds[i], method=\"bilinear\", antialias=True,\n",
    "                    name=f\"enc_down_L{i+1}_{str(ds[i]).replace('.','p')}\",\n",
    "                    dtype=\"float32\")\n",
    "        for i in range(4)\n",
    "    ]\n",
    "    up_to = ResizeToMatch(method=\"bilinear\", antialias=True, name=\"dec_up\", dtype=\"float32\")\n",
    "    final_2x = ResizeByScale(1.0, method=\"bilinear\", antialias=True, name=\"final_2x\")\n",
    "\n",
    "    # Encoder\n",
    "    s1, p1 = encoder_block(inputs,  64, down_layer=down_layers[0])\n",
    "    s2, p2 = encoder_block(p1,    128, down_layer=down_layers[1])\n",
    "    s3, p3 = encoder_block(p2,    256, down_layer=down_layers[2])\n",
    "    s4, p4 = encoder_block(p3,    512, down_layer=down_layers[3])\n",
    "\n",
    "    # Bridge\n",
    "    b1 = conv_block(p4, 1024)\n",
    "\n",
    "    # Decoder\n",
    "    d1 = decoder_block(b1, s4, 512, up_to_layer=up_to)\n",
    "    d2 = decoder_block(d1, s3, 256, up_to_layer=up_to)\n",
    "    d3 = decoder_block(d2, s2, 128, up_to_layer=up_to)\n",
    "    d4 = decoder_block(d3, s1,  64, up_to_layer=up_to)\n",
    "\n",
    "    # Final SR head (2x)\n",
    "    u1 = final_2x(d4)\n",
    "    u1 = L.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(u1)\n",
    "    u1 = conv_block(u1, 64)\n",
    "\n",
    "    outputs = L.Activation(\"sigmoid\")(L.Conv2D(3, 1, padding=\"same\")(u1))\n",
    "\n",
    "    if name is None:\n",
    "        tag = \"-\".join([str(x).replace('.', 'p') for x in ds])\n",
    "        name = f\"UNet_SR_ds_{tag}\"  # safe name (no brackets)\n",
    "    return Model(inputs, outputs, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:03:40.700266Z",
     "iopub.status.busy": "2022-03-14T05:03:40.699702Z",
     "iopub.status.idle": "2022-03-14T05:03:40.704884Z",
     "shell.execute_reply": "2022-03-14T05:03:40.704018Z",
     "shell.execute_reply.started": "2022-03-14T05:03:40.700229Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "\n",
    "vgg = VGG19(include_top=False, weights=\"imagenet\", input_shape=(256, 256, 3))\n",
    "vgg.trainable = False\n",
    "feat_extractor = tf.keras.Model(\n",
    "    inputs=vgg.input,\n",
    "    outputs=vgg.get_layer(\"block4_conv4\").output,\n",
    ")\n",
    "\n",
    "alpha = tf.constant(1.0,  dtype=tf.float32)\n",
    "beta  = tf.constant(0.1,  dtype=tf.float32)\n",
    "gamma = tf.constant(0.01, dtype=tf.float32)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    val = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    return tf.cast(val, tf.float32)\n",
    "\n",
    "def ssim_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    val = 1.0 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=1.0))\n",
    "    return tf.cast(val, tf.float32)\n",
    "\n",
    "def perceptual_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(tf.clip_by_value(y_true, 0.0, 1.0), tf.float32)\n",
    "    y_pred = tf.cast(tf.clip_by_value(y_pred, 0.0, 1.0), tf.float32)\n",
    "    feat_true = feat_extractor(preprocess_input(y_true * 255.0))\n",
    "    feat_pred = feat_extractor(preprocess_input(y_pred * 255.0))\n",
    "    val = tf.reduce_mean(tf.square(feat_true - feat_pred))\n",
    "    return tf.cast(val, tf.float32)\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    total = (\n",
    "        alpha * mse_loss(y_true, y_pred)\n",
    "        + beta  * ssim_loss(y_true, y_pred)\n",
    "        + gamma * perceptual_loss(y_true, y_pred)\n",
    "    )\n",
    "    return tf.cast(total, tf.float32)\n",
    "\n",
    "def psnr_metric(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(tf.clip_by_value(y_pred, 0.0, 1.0), tf.float32)\n",
    "    return tf.reduce_mean(tf.image.psnr(y_true, y_pred, max_val=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python exec: /Users/kunalnarwani/Desktop/Thesis/super-resolution/.venv/bin/python\n",
      "TF version: 2.15.1\n",
      "GPUs: []\n"
     ]
    }
   ],
   "source": [
    "# ============================== Optimizer / compile ========================= #\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# ===== Compile =====\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=[psnr_metric],\n",
    ")\n",
    "\n",
    "print(\"python exec:\", sys.executable)\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"GPUs:\", tf.config.list_physical_devices(\"GPU\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:03:41.632394Z",
     "iopub.status.busy": "2022-03-14T05:03:41.631893Z",
     "iopub.status.idle": "2022-03-14T05:10:07.166952Z",
     "shell.execute_reply": "2022-03-14T05:10:07.166144Z",
     "shell.execute_reply.started": "2022-03-14T05:03:41.632348Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.AdamFloat32` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.AdamFloat32`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training combo 0p25-0p25-0p25-0p25 (downscales=[0.25, 0.25, 0.25, 0.25]) ===\n",
      "Epoch 1/60\n",
      "\n",
      "Epoch 1: val_psnr_metric improved from -inf to 11.65326, saving model to models/best_by_psnr_ds_0p25-0p25-0p25-0p25.keras\n",
      "87/87 - 628s - loss: nan - psnr_metric: 20.5562 - val_loss: nan - val_psnr_metric: 11.6533 - 628s/epoch - 7s/step\n",
      "Epoch 2/60\n",
      "\n",
      "Epoch 2: val_psnr_metric improved from 11.65326 to 12.70810, saving model to models/best_by_psnr_ds_0p25-0p25-0p25-0p25.keras\n",
      "87/87 - 591s - loss: nan - psnr_metric: 22.6161 - val_loss: nan - val_psnr_metric: 12.7081 - 591s/epoch - 7s/step\n",
      "Epoch 3/60\n",
      "\n",
      "Epoch 3: val_psnr_metric improved from 12.70810 to 14.06318, saving model to models/best_by_psnr_ds_0p25-0p25-0p25-0p25.keras\n",
      "87/87 - 573s - loss: nan - psnr_metric: 24.2725 - val_loss: nan - val_psnr_metric: 14.0632 - 573s/epoch - 7s/step\n",
      "Epoch 4/60\n"
     ]
    }
   ],
   "source": [
    "# =========================== Data pipelines ============================ #\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def make_tfds(lr_np, hr_np, batch_size=8, shuffle=True, repeat=False):\n",
    "    assert len(lr_np) == len(hr_np), \"LR/HR length mismatch\"\n",
    "    ds = tf.data.Dataset.from_tensor_slices((lr_np, hr_np))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=min(8_192, len(lr_np)), reshuffle_each_iteration=True)\n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "    ds = ds.batch(batch_size, drop_remainder=False)\n",
    "    ds = ds.prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "train_ds = make_tfds(train_low_image, train_high_image, batch_size=BATCH_SIZE, shuffle=True, repeat=True)\n",
    "valid_ds = make_tfds(validation_low_image, validation_high_image, batch_size=BATCH_SIZE, shuffle=False, repeat=False)\n",
    "test_ds  = make_tfds(test_low_image,  test_high_image,  batch_size=BATCH_SIZE, shuffle=False, repeat=False)\n",
    "\n",
    "steps_per_epoch    = max(1, len(train_low_image) // BATCH_SIZE)\n",
    "validation_steps   = max(1, len(validation_low_image) // BATCH_SIZE)\n",
    "\n",
    "# ======================== Optimizer & compile ========================== #\n",
    "# Keep optimizer in float32 when using mixed precision\n",
    "class AdamFloat32(tf.keras.optimizers.Adam):\n",
    "    @tf.function(jit_compile=False)\n",
    "    def _resource_apply_dense(self, grad, var, apply_state=None):\n",
    "        return super()._resource_apply_dense(tf.cast(grad, tf.float32), var, apply_state)\n",
    "\n",
    "def compile_model(model, lr=2e-4):\n",
    "    opt = AdamFloat32(learning_rate=lr)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss=combined_loss,         # from your code\n",
    "                  metrics=[psnr_metric])      # from your code\n",
    "    return model\n",
    "\n",
    "# ========================== Callbacks ================================= #\n",
    "def make_callbacks(tag):\n",
    "    ckpt_path = f\"models/best_by_psnr_ds_{tag}.keras\"\n",
    "    os.makedirs(os.path.dirname(ckpt_path), exist_ok=True)\n",
    "    early_stop = EarlyStopping(monitor='val_psnr_metric', mode='max',\n",
    "                               patience=10, restore_best_weights=True, verbose=1)\n",
    "    model_ckpt = ModelCheckpoint(filepath=ckpt_path, monitor='val_psnr_metric',\n",
    "                                 mode='max', save_best_only=True, verbose=1)\n",
    "    return [early_stop, model_ckpt], ckpt_path\n",
    "\n",
    "# ======================= Train one configuration ====================== #\n",
    "def train_one_combo(downscales, epochs=100, base_lr=2e-4, tag=None, verbose=2):\n",
    "    ds_list = [float(x) for x in (downscales if isinstance(downscales, (list, tuple)) else [downscales]*4)]\n",
    "    if tag is None:\n",
    "        tag = \"-\".join([str(x).replace('.', 'p') for x in ds_list])\n",
    "\n",
    "    model = build_super_resolution_unet(input_shape=(SIZE, SIZE, 3), downscales=ds_list, name=f\"UNet_SR_ds_{tag}\")\n",
    "    compile_model(model, lr=base_lr)\n",
    "    cbs, ckpt_path = make_callbacks(tag)\n",
    "\n",
    "    print(f\"\\n=== Training combo {tag} (downscales={ds_list}) ===\")\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=valid_ds,\n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=cbs,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Evaluate best weights on validation (loaded by EarlyStopping restore) and on test\n",
    "    val_psnr = max(history.history.get('val_psnr_metric', [-float(\"inf\")]))\n",
    "    print(f\"[{tag}] Best val PSNR: {val_psnr:.3f} dB\")\n",
    "\n",
    "    # Load the *saved* best checkpoint explicitly (in case restore_best_weights didn't coincide with checkpoint)\n",
    "    try:\n",
    "        best_model = tf.keras.models.load_model(\n",
    "            ckpt_path,\n",
    "            custom_objects={\n",
    "                \"ResizeByScale\": ResizeByScale,\n",
    "                \"ResizeToMatch\": ResizeToMatch,\n",
    "                \"combined_loss\": combined_loss,\n",
    "                \"psnr_metric\": psnr_metric\n",
    "            },\n",
    "            compile=False,\n",
    "            safe_mode=False\n",
    "        )\n",
    "        compile_model(best_model, lr=base_lr)  # compile for evaluation metrics\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: failed to load checkpoint {ckpt_path}: {e}\")\n",
    "        best_model = model  # fallback\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_metrics = best_model.evaluate(test_ds, verbose=0, return_dict=True)\n",
    "    print(f\"[{tag}] Test PSNR: {test_metrics.get('psnr_metric', float('nan')):.3f} dB\")\n",
    "\n",
    "    # Optionally compute SSIM over test set (full-image average)\n",
    "    ssim_vals = []\n",
    "    for lr_batch, hr_batch in test_ds:\n",
    "        pred = best_model.predict(lr_batch, verbose=0)\n",
    "        pred = tf.clip_by_value(pred, 0.0, 1.0)\n",
    "        ssim = tf.image.ssim(tf.cast(hr_batch, tf.float32), tf.cast(pred, tf.float32), max_val=1.0)\n",
    "        ssim_vals.append(ssim)\n",
    "    test_ssim = float(tf.reduce_mean(tf.concat(ssim_vals, axis=0)))\n",
    "    print(f\"[{tag}] Test SSIM: {test_ssim:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"tag\": tag,\n",
    "        \"downscales\": ds_list,\n",
    "        \"val_best_psnr\": float(val_psnr),\n",
    "        \"test_psnr\": float(test_metrics.get('psnr_metric', float('nan'))),\n",
    "        \"test_ssim\": test_ssim,\n",
    "        \"ckpt_path\": ckpt_path\n",
    "    }\n",
    "\n",
    "# ========================= Grid search  ===================== #\n",
    "from itertools import product\n",
    "\n",
    "def grid_search_scales(level_choices=(0.25, 0.5, 0.75), limit=None, epochs=60, base_lr=2e-4, verbose=2):\n",
    "    combos = list(product(level_choices, repeat=4))\n",
    "    if limit is not None:\n",
    "        combos = combos[:int(limit)]\n",
    "    results = []\n",
    "    best = {\"val_best_psnr\": -1.0}\n",
    "    for ds in combos:\n",
    "        tag = \"-\".join([str(x).replace('.', 'p') for x in ds])\n",
    "        res = train_one_combo(list(ds), epochs=epochs, base_lr=base_lr, tag=tag, verbose=verbose)\n",
    "        results.append(res)\n",
    "        if res[\"val_best_psnr\"] > best[\"val_best_psnr\"]:\n",
    "            best = res\n",
    "    print(\"\\n=== Grid search done ===\")\n",
    "    print(f\"Best by val PSNR: {best['val_best_psnr']:.3f} dB @ {best['downscales']} (ckpt: {best['ckpt_path']})\")\n",
    "    return results, best\n",
    "\n",
    "# ============================ Run training ============================ #\n",
    "\n",
    "\n",
    "results, best = grid_search_scales(level_choices=(0.25, 0.5, 0.75), limit=None, epochs=60, base_lr=2e-4, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:10:08.297239Z",
     "iopub.status.busy": "2022-03-14T05:10:08.297006Z",
     "iopub.status.idle": "2022-03-14T05:10:08.304194Z",
     "shell.execute_reply": "2022-03-14T05:10:08.303442Z",
     "shell.execute_reply.started": "2022-03-14T05:10:08.297212Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m n_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lr_b, hr_b \u001b[38;5;129;01min\u001b[39;00m eval_ds:\n\u001b[0;32m---> 12\u001b[0m     pred_b \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pred_b\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m!=\u001b[39m hr_b\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m3\u001b[39m]:  \u001b[38;5;66;03m# still guards future changes\u001b[39;00m\n\u001b[1;32m     15\u001b[0m         pred_b \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mresize(pred_b, size\u001b[38;5;241m=\u001b[39mhr_b\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m3\u001b[39m], method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:590\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39mcopied_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcopied_kwargs)\n\u001b[1;32m    588\u001b[0m     layout_map_lib\u001b[38;5;241m.\u001b[39m_map_subclass_model_variable(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layout_map)\n\u001b[0;32m--> 590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/keras/src/engine/base_layer.py:1149\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1148\u001b[0m ):\n\u001b[0;32m-> 1149\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/keras/src/engine/functional.py:515\u001b[0m, in \u001b[0;36mFunctional.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;129m@doc_controls\u001b[39m\u001b[38;5;241m.\u001b[39mdo_not_doc_inheritable\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    498\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \n\u001b[1;32m    500\u001b[0m \u001b[38;5;124;03m    In this case `call` just reapplies\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;124;03m        a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 515\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_internal_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/keras/src/engine/functional.py:672\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[1;32m    671\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[0;32m--> 672\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;66;03m# Update tensor_dict.\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_id, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    676\u001b[0m     node\u001b[38;5;241m.\u001b[39mflat_output_ids, tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(outputs)\n\u001b[1;32m    677\u001b[0m ):\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/keras/src/engine/base_layer.py:1149\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1148\u001b[0m ):\n\u001b[0;32m-> 1149\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:290\u001b[0m, in \u001b[0;36mConv.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    286\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compiled_convolution_op(\n\u001b[1;32m    287\u001b[0m         inputs, tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel)\n\u001b[1;32m    288\u001b[0m     )\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 290\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolution_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[1;32m    293\u001b[0m     output_rank \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:262\u001b[0m, in \u001b[0;36mConv.convolution_op\u001b[0;34m(self, inputs, kernel)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    260\u001b[0m     tf_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrides\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf_padding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdilations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation_rate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tf_data_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/tensorflow/python/ops/nn_ops.py:1186\u001b[0m, in \u001b[0;36mconvolution_v2\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnn.convolution\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconvolution_v2\u001b[39m(  \u001b[38;5;66;03m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1184\u001b[0m     dilations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1185\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1186\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvolution_internal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=redefined-builtin\u001b[39;49;00m\n\u001b[1;32m   1188\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdilations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdilations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/tensorflow/python/ops/nn_ops.py:1319\u001b[0m, in \u001b[0;36mconvolution_internal\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[1;32m   1316\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1317\u001b[0m     op \u001b[38;5;241m=\u001b[39m conv1d\n\u001b[0;32m-> 1319\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdilations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdilations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1328\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m channel_index \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/tensorflow/python/ops/nn_ops.py:2793\u001b[0m, in \u001b[0;36m_conv2d_expanded_batch\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   2789\u001b[0m input_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n\u001b[1;32m   2790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m input_rank \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[1;32m   2791\u001b[0m   \u001b[38;5;66;03m# We avoid calling squeeze_batch_dims to reduce extra python function\u001b[39;00m\n\u001b[1;32m   2792\u001b[0m   \u001b[38;5;66;03m# call slowdown in eager mode.  This branch doesn't require reshapes.\u001b[39;00m\n\u001b[0;32m-> 2793\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_nn_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2794\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2795\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2796\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2797\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2798\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2799\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdilations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdilations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2800\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2801\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m squeeze_batch_dims(\n\u001b[1;32m   2802\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2803\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2810\u001b[0m     inner_rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m   2811\u001b[0m     name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_nn_ops.py:1345\u001b[0m, in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1343\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1345\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconv2d_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cudnn_on_gpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cudnn_on_gpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplicit_paddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplicit_paddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdilations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[1;32m   1350\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_nn_ops.py:1434\u001b[0m, in \u001b[0;36mconv2d_eager_fallback\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name, ctx)\u001b[0m\n\u001b[1;32m   1430\u001b[0m _inputs_flat \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mfilter\u001b[39m]\n\u001b[1;32m   1431\u001b[0m _attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_T, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrides\u001b[39m\u001b[38;5;124m\"\u001b[39m, strides, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cudnn_on_gpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1432\u001b[0m use_cudnn_on_gpu, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpadding\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexplicit_paddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1433\u001b[0m explicit_paddings, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_format\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_format, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdilations\u001b[39m\u001b[38;5;124m\"\u001b[39m, dilations)\n\u001b[0;32m-> 1434\u001b[0m _result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mConv2D\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_inputs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _execute\u001b[38;5;241m.\u001b[39mmust_record_gradient():\n\u001b[1;32m   1437\u001b[0m   _execute\u001b[38;5;241m.\u001b[39mrecord_gradient(\n\u001b[1;32m   1438\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConv2D\u001b[39m\u001b[38;5;124m\"\u001b[39m, _inputs_flat, _attrs, _result)\n",
      "File \u001b[0;32m~/Desktop/Thesis/super-resolution/.venv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EVAL_BATCH = 8  # adjust for GPU/CPU memory\n",
    "eval_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((validation_low_image, validation_high_image))\n",
    "      .batch(EVAL_BATCH)\n",
    "      .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "all_psnr, all_ssim, all_msssim = [], [], []\n",
    "n_images = 0\n",
    "\n",
    "for lr_b, hr_b in eval_ds:\n",
    "    pred_b = model(lr_b, training=False)\n",
    "\n",
    "    if pred_b.shape[1:3] != hr_b.shape[1:3]:  # still guards future changes\n",
    "        pred_b = tf.image.resize(pred_b, size=hr_b.shape[1:3], method=\"bicubic\")\n",
    "\n",
    "    hr_tf   = tf.cast(hr_b, tf.float32)\n",
    "    pred_tf = tf.cast(tf.clip_by_value(pred_b, 0.0, 1.0), tf.float32)\n",
    "\n",
    "    all_psnr.append(tf.image.psnr(hr_tf, pred_tf, max_val=1.0).numpy())\n",
    "    all_ssim.append(tf.image.ssim(hr_tf, pred_tf, max_val=1.0).numpy())\n",
    "    all_msssim.append(tf.image.ssim_multiscale(hr_tf, pred_tf, max_val=1.0).numpy())\n",
    "\n",
    "    n_images += int(hr_b.shape[0])\n",
    "\n",
    "def mean_std(x):\n",
    "    x = np.concatenate(x, axis=0).astype(np.float64)\n",
    "    return float(np.mean(x)), float(np.std(x))\n",
    "\n",
    "m_psnr, s_psnr   = mean_std(all_psnr)\n",
    "m_ssim, s_ssim   = mean_std(all_ssim)\n",
    "m_msssim, s_msssim = mean_std(all_msssim)\n",
    "\n",
    "print(f\"Validation images evaluated: {n_images}\")\n",
    "print(f\" PSNR    : {m_psnr:.4f} ± {s_psnr:.4f} dB\")\n",
    "print(f\" SSIM    : {m_ssim:.4f} ± {s_ssim:.4f}\")\n",
    "print(f\" MS-SSIM : {m_msssim:.4f} ± {s_msssim:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
